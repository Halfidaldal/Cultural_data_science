{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b16e3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries and Configure Environment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import kagglehub\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Configure Plotting\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Check for GPU\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cba224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Path to dataset files: /Users/halfidaldal/.cache/kagglehub/datasets/rhaamrozenberg/billboards-top-100-song-1946-to-2022-lyrics/versions/1\n",
      "Loaded 6879 rows.\n",
      "Original Columns: Index(['Song', 'Artist Names', 'Hot100 Ranking Year', 'Hot100 Rank', 'Lyrics'], dtype='object')\n",
      "Renamed Columns: Index(['title', 'artist', 'year', 'rank', 'lyrics'], dtype='object')\n",
      "Cleaning lyrics...\n",
      "Path to dataset files: /Users/halfidaldal/.cache/kagglehub/datasets/rhaamrozenberg/billboards-top-100-song-1946-to-2022-lyrics/versions/1\n",
      "Loaded 6879 rows.\n",
      "Original Columns: Index(['Song', 'Artist Names', 'Hot100 Ranking Year', 'Hot100 Rank', 'Lyrics'], dtype='object')\n",
      "Renamed Columns: Index(['title', 'artist', 'year', 'rank', 'lyrics'], dtype='object')\n",
      "Cleaning lyrics...\n",
      "Lyrics length stats: count    6879.000000\n",
      "mean     1399.715075\n",
      "std       943.656294\n",
      "min         0.000000\n",
      "25%       819.500000\n",
      "50%      1293.000000\n",
      "75%      1863.000000\n",
      "max      6036.000000\n",
      "Name: lyrics_clean, dtype: float64\n",
      "Rows after lyrics length filter: 6029 (dropped 850)\n",
      "Rows after year filter: 2134 (dropped 3895)\n",
      "Final dataset size: 2134\n",
      "                                    title                          artist  \\\n",
      "4585                              Breathe                  ['faith hill']   \n",
      "4586            Smooth (feat. Rob Thomas)       ['santana', 'rob thomas']   \n",
      "4587  Maria Maria (feat. The Product G&B)  ['santana', 'the product g b']   \n",
      "4588                         I Wanna Know                         ['joe']   \n",
      "4589                  Everything You Want            ['vertical horizon']   \n",
      "\n",
      "      year  rank                                             lyrics  \\\n",
      "4585  2000     1  ['i', 'can', 'feel', 'the', 'magic', 'floating...   \n",
      "4586  2000     2  ['man', 'its', 'a', 'hot', 'one', 'like', 'sev...   \n",
      "4587  2000     3  ['ladies', 'and', 'gents', 'turn', 'up', 'your...   \n",
      "4588  2000     4  ['yeah', 'oh', 'yeah', 'alright', 'oh', 'oh', ...   \n",
      "4589  2000     5  ['somewhere', 'theres', 'speaking', 'its', 'al...   \n",
      "\n",
      "                                           lyrics_clean  \n",
      "4585  i can feel the magic floating in the air being...  \n",
      "4586  man its a hot one like seven inches from the m...  \n",
      "4587  ladies and gents turn up your sound system to ...  \n",
      "4588  yeah oh yeah alright oh oh oh its amazing how ...  \n",
      "4589  somewhere theres speaking its already coming i...  \n",
      "Lyrics length stats: count    6879.000000\n",
      "mean     1399.715075\n",
      "std       943.656294\n",
      "min         0.000000\n",
      "25%       819.500000\n",
      "50%      1293.000000\n",
      "75%      1863.000000\n",
      "max      6036.000000\n",
      "Name: lyrics_clean, dtype: float64\n",
      "Rows after lyrics length filter: 6029 (dropped 850)\n",
      "Rows after year filter: 2134 (dropped 3895)\n",
      "Final dataset size: 2134\n",
      "                                    title                          artist  \\\n",
      "4585                              Breathe                  ['faith hill']   \n",
      "4586            Smooth (feat. Rob Thomas)       ['santana', 'rob thomas']   \n",
      "4587  Maria Maria (feat. The Product G&B)  ['santana', 'the product g b']   \n",
      "4588                         I Wanna Know                         ['joe']   \n",
      "4589                  Everything You Want            ['vertical horizon']   \n",
      "\n",
      "      year  rank                                             lyrics  \\\n",
      "4585  2000     1  ['i', 'can', 'feel', 'the', 'magic', 'floating...   \n",
      "4586  2000     2  ['man', 'its', 'a', 'hot', 'one', 'like', 'sev...   \n",
      "4587  2000     3  ['ladies', 'and', 'gents', 'turn', 'up', 'your...   \n",
      "4588  2000     4  ['yeah', 'oh', 'yeah', 'alright', 'oh', 'oh', ...   \n",
      "4589  2000     5  ['somewhere', 'theres', 'speaking', 'its', 'al...   \n",
      "\n",
      "                                           lyrics_clean  \n",
      "4585  i can feel the magic floating in the air being...  \n",
      "4586  man its a hot one like seven inches from the m...  \n",
      "4587  ladies and gents turn up your sound system to ...  \n",
      "4588  yeah oh yeah alright oh oh oh its amazing how ...  \n",
      "4589  somewhere theres speaking its already coming i...  \n"
     ]
    }
   ],
   "source": [
    "# 2. Load Data and Preprocess Lyrics\n",
    "\n",
    "# Download/Load Dataset\n",
    "print(\"Loading dataset...\")\n",
    "path = kagglehub.dataset_download(\"rhaamrozenberg/billboards-top-100-song-1946-to-2022-lyrics\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "# Load CSV\n",
    "import os\n",
    "import ast\n",
    "csv_path = os.path.join(path, \"Billboard_Hot_100_with_features.csv\") # Verify filename if possible, usually it's this or similar\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    files = os.listdir(path)\n",
    "    csv_files = [f for f in files if f.endswith('.csv')]\n",
    "    if csv_files:\n",
    "        csv_path = os.path.join(path, csv_files[0])\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No CSV found in dataset path\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Loaded {len(df)} rows.\")\n",
    "print(\"Original Columns:\", df.columns)\n",
    "\n",
    "# Rename columns for consistency\n",
    "# Actual columns: ['Song', 'Artist Names', 'Hot100 Ranking Year', 'Hot100 Rank', 'Lyrics']\n",
    "rename_map = {\n",
    "    'Hot100 Ranking Year': 'year',\n",
    "    'Hot100 Rank': 'rank',\n",
    "    'Lyrics': 'lyrics',\n",
    "    'Song': 'title',\n",
    "    'Artist Names': 'artist'\n",
    "}\n",
    "df.rename(columns=rename_map, inplace=True)\n",
    "print(\"Renamed Columns:\", df.columns)\n",
    "\n",
    "# Ensure numeric types\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "df['rank'] = pd.to_numeric(df['rank'], errors='coerce')\n",
    "\n",
    "# Basic Preprocessing\n",
    "# 1. Convert 'year' to numeric if needed (it usually is)\n",
    "# 2. Clean lyrics\n",
    "def clean_lyrics(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # The lyrics seem to be string representations of lists: \"['word', 'word', ...]\"\n",
    "    # We need to parse this string back into a list and join it, or just clean the string representation.\n",
    "    # Let's try to parse it safely.\n",
    "    try:\n",
    "        # If it looks like a list string \"['...']\"\n",
    "        if text.strip().startswith('[') and text.strip().endswith(']'):\n",
    "            # Use ast.literal_eval to safely parse the string list\n",
    "            words = ast.literal_eval(text)\n",
    "            if isinstance(words, list):\n",
    "                text = ' '.join(words)\n",
    "    except (ValueError, SyntaxError):\n",
    "        # If parsing fails, fall back to regex cleaning of the string representation\n",
    "        pass\n",
    "\n",
    "    text = text.lower()\n",
    "    # Remove text in brackets like [Chorus], [Verse 1] (if any remain after joining)\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    # Remove special characters but keep basic punctuation if needed, or strip all\n",
    "    text = re.sub(r'[^a-z0-9\\s\\.,\\'\\?!]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "print(\"Cleaning lyrics...\")\n",
    "df['lyrics_clean'] = df['lyrics'].apply(clean_lyrics)\n",
    "\n",
    "# Debug: Check lengths\n",
    "print(\"Lyrics length stats:\", df['lyrics_clean'].str.len().describe())\n",
    "\n",
    "# Filter empty lyrics\n",
    "before_len = len(df)\n",
    "df = df[df['lyrics_clean'].str.len() > 50]\n",
    "print(f\"Rows after lyrics length filter: {len(df)} (dropped {before_len - len(df)})\")\n",
    "\n",
    "# Filter Year Window (2000-2023 is the dataset, but let's ensure)\n",
    "before_len = len(df)\n",
    "df = df[(df['year'] >= 2000) & (df['year'] <= 2023)]\n",
    "print(f\"Rows after year filter: {len(df)} (dropped {before_len - len(df)})\")\n",
    "\n",
    "# Sort\n",
    "df = df.sort_values(['year', 'rank'])\n",
    "\n",
    "print(f\"Final dataset size: {len(df)}\")\n",
    "if len(df) > 0:\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"WARNING: Dataset is empty after filtering. Check year range and lyrics content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6201dec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Sentiment Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Emotion Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520d32f4e8b4413aaf47733798680c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5ab0c0373d4d2dbe99b2a42b9e6bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PENPAL_analysis/venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:626\u001b[39m, in \u001b[36mxet_get\u001b[39m\u001b[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[39m\n\u001b[32m    624\u001b[39m     progress.update(progress_bytes)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Initialize Emotion Pipeline (GoEmotions)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInitializing Emotion Model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m emotion_pipeline = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-classification\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSamLowe/roberta-base-go_emotions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Function to process in batches\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_sentiment_emotion\u001b[39m(texts, batch_size=\u001b[32m32\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PENPAL_analysis/venv/lib/python3.13/site-packages/transformers/pipelines/__init__.py:942\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    941\u001b[39m     model_classes = {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     framework, model = \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    952\u001b[39m model_config = model.config\n\u001b[32m    953\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PENPAL_analysis/venv/lib/python3.13/site-packages/transformers/pipelines/base.py:291\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m     logger.warning(\n\u001b[32m    286\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    287\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTrying to load the model with Tensorflow.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    288\u001b[39m     )\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     model = \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    293\u001b[39m         model = model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PENPAL_analysis/venv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PENPAL_analysis/venv/lib/python3.13/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PENPAL_analysis/venv/lib/python3.13/site-packages/transformers/modeling_utils.py:4260\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4251\u001b[39m     gguf_file\n\u001b[32m   4252\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4253\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4254\u001b[39m ):\n\u001b[32m   4255\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4256\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4260\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4262\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4267\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4273\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4276\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4278\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4279\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PENPAL_analysis/venv/lib/python3.13/site-packages/transformers/modeling_utils.py:997\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[39m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    983\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m    984\u001b[39m     cached_file_kwargs = {\n\u001b[32m    985\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcache_dir\u001b[39m\u001b[33m\"\u001b[39m: cache_dir,\n\u001b[32m    986\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mforce_download\u001b[39m\u001b[33m\"\u001b[39m: force_download,\n\u001b[32m   (...)\u001b[39m\u001b[32m    995\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m: commit_hash,\n\u001b[32m    996\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m     resolved_archive_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    999\u001b[39m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[32m   1000\u001b[39m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[32m   1001\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[32m   1002\u001b[39m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PENPAL_analysis/venv/lib/python3.13/site-packages/transformers/utils/hub.py:266\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    209\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    210\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    211\u001b[39m     **kwargs,\n\u001b[32m    212\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    213\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PENPAL_analysis/venv/lib/python3.13/site-packages/transformers/utils/hub.py:424\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    422\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    423\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    439\u001b[39m         snapshot_download(\n\u001b[32m    440\u001b[39m             path_or_repo_id,\n\u001b[32m    441\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    451\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PENPAL_analysis/venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PENPAL_analysis/venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    988\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    989\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1005\u001b[39m     )\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PENPAL_analysis/venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1168\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1180\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1181\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PENPAL_analysis/venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1720\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1718\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[32m   1719\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1720\u001b[39m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1721\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1726\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1727\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1728\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants.HF_HUB_DISABLE_XET:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PENPAL_analysis/venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:621\u001b[39m, in \u001b[36mxet_get\u001b[39m\u001b[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[39m\n\u001b[32m    610\u001b[39m     displayed_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_filename[:\u001b[32m40\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(â€¦)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    612\u001b[39m progress_cm = _get_progress_bar_context(\n\u001b[32m    613\u001b[39m     desc=displayed_filename,\n\u001b[32m    614\u001b[39m     log_level=logger.getEffectiveLevel(),\n\u001b[32m   (...)\u001b[39m\u001b[32m    618\u001b[39m     _tqdm_bar=_tqdm_bar,\n\u001b[32m    619\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m progress_cm \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprogress_updater\u001b[39m(progress_bytes: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[32m    624\u001b[39m         progress.update(progress_bytes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PENPAL_analysis/venv/lib/python3.13/site-packages/tqdm/std.py:1138\u001b[39m, in \u001b[36mtqdm.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, traceback):\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 3. Extract Sentiment and Emotion Features\n",
    "\n",
    "# Initialize Sentiment Pipeline (RoBERTa)\n",
    "print(\"Initializing Sentiment Model...\")\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    device=0 if device == 0 else -1, # MPS support in transformers pipelines can be tricky, usually CPU or CUDA\n",
    "    top_k=None # Return all scores\n",
    ")\n",
    "\n",
    "# Initialize Emotion Pipeline (GoEmotions)\n",
    "print(\"Initializing Emotion Model...\")\n",
    "emotion_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"SamLowe/roberta-base-go_emotions\",\n",
    "    device=0 if device == 0 else -1,\n",
    "    top_k=None\n",
    ")\n",
    "\n",
    "# Function to process in batches\n",
    "def get_sentiment_emotion(texts, batch_size=32):\n",
    "    sentiments = []\n",
    "    emotions = []\n",
    "    \n",
    "    # Process sentiment\n",
    "    print(\"Processing Sentiment...\")\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        # Enable truncation to prevent CUDA errors on long lyrics\n",
    "        results = sentiment_pipeline(batch, truncation=True, max_length=512)\n",
    "        # results is list of lists of dicts\n",
    "        for res in results:\n",
    "            # res is [{'label': 'positive', 'score': 0.9}, ...]\n",
    "            scores = {item['label']: item['score'] for item in res}\n",
    "            # Calculate valence score: Positive - Negative\n",
    "            valence = scores.get('positive', 0) - scores.get('negative', 0)\n",
    "            sentiments.append(valence)\n",
    "            \n",
    "    # Process emotions\n",
    "    print(\"Processing Emotions...\")\n",
    "    # We'll focus on key emotions: sadness, anxiety (fear), anger, joy\n",
    "    target_emotions = ['sadness', 'fear', 'anger', 'joy', 'optimism', 'excitement']\n",
    "    \n",
    "    emotion_data = {e: [] for e in target_emotions}\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        results = emotion_pipeline(batch, truncation=True, max_length=512)\n",
    "        \n",
    "        for res in results:\n",
    "            scores = {item['label']: item['score'] for item in res}\n",
    "            for e in target_emotions:\n",
    "                emotion_data[e].append(scores.get(e, 0))\n",
    "                \n",
    "    return sentiments, emotion_data\n",
    "\n",
    "# Run extraction\n",
    "# Using a sample for speed if needed, but let's try full dataset (approx 2300 songs)\n",
    "texts = df['lyrics_clean'].tolist()\n",
    "print(f\"Extracting features for {len(texts)} songs...\")\n",
    "\n",
    "# Note: This might take a few minutes.\n",
    "valences, emotion_scores = get_sentiment_emotion(texts)\n",
    "\n",
    "# Add to DataFrame\n",
    "df['valence_transformer'] = valences\n",
    "for e, scores in emotion_scores.items():\n",
    "    df[f'emotion_{e}'] = scores\n",
    "\n",
    "print(\"Feature extraction complete.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e60fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Extract Lexical and Economic Dictionary Features\n",
    "\n",
    "# Define Lexicons\n",
    "economic_lexicon = set([\n",
    "    'money', 'broke', 'job', 'work', 'bills', 'rent', 'debt', 'pay', 'paycheck', \n",
    "    'mortgage', 'bank', 'rich', 'poor', 'cash', 'dollar', 'price', 'cost', 'economy',\n",
    "    'crisis', 'recession', 'hustle', 'grind', 'wage', 'saving', 'spend'\n",
    "])\n",
    "\n",
    "pronouns_first = set(['i', 'me', 'my', 'mine', 'myself'])\n",
    "pronouns_social = set(['we', 'us', 'our', 'ours', 'ourselves'])\n",
    "negations = set(['not', 'never', 'no', 'cant', 'wont', 'dont', 'didnt', 'couldnt', 'wouldnt', 'shouldnt'])\n",
    "\n",
    "def extract_lexical_features(text):\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    if num_words == 0:\n",
    "        return pd.Series([0]*6)\n",
    "    \n",
    "    # Counts\n",
    "    econ_count = sum(1 for w in words if w in economic_lexicon)\n",
    "    first_person_count = sum(1 for w in words if w in pronouns_first)\n",
    "    social_count = sum(1 for w in words if w in pronouns_social)\n",
    "    negation_count = sum(1 for w in words if w in negations)\n",
    "    \n",
    "    # Type-Token Ratio\n",
    "    ttr = len(set(words)) / num_words\n",
    "    \n",
    "    # Avg Word Length\n",
    "    avg_len = sum(len(w) for w in words) / num_words\n",
    "    \n",
    "    return pd.Series([\n",
    "        econ_count / num_words, # Normalize by length\n",
    "        first_person_count / num_words,\n",
    "        social_count / num_words,\n",
    "        negation_count / num_words,\n",
    "        ttr,\n",
    "        avg_len\n",
    "    ])\n",
    "\n",
    "print(\"Extracting lexical features...\")\n",
    "lexical_cols = ['freq_economic', 'freq_self', 'freq_social', 'freq_negation', 'ttr', 'avg_word_len']\n",
    "df[lexical_cols] = df['lyrics_clean'].apply(extract_lexical_features)\n",
    "\n",
    "print(\"Lexical extraction complete.\")\n",
    "df[lexical_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f88f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Compute Semantic Similarity to Recession Concepts\n",
    "\n",
    "# Initialize Sentence Transformer\n",
    "# 'all-mpnet-base-v2' is a strong general-purpose model\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "print(f\"Loading Embedding model: {EMBEDDING_MODEL}\")\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL, device=device if device != -1 else 'cpu')\n",
    "\n",
    "# Define rich concept descriptions\n",
    "concepts = {\n",
    "    \"econ_anxiety\": \"lyrics about losing jobs, struggling to pay bills, being broke, worrying about money, recession, crisis\",\n",
    "    \"resilience\": \"working overtime, hustling to survive, keeping family afloat, working multiple jobs\",\n",
    "    \"escapist_party\": \"forgetting problems by partying, drinking, getting high to escape real life\",\n",
    "    \"luxury_flex\": \"bragging about money, expensive cars, brands, being rich\"\n",
    "}\n",
    "\n",
    "# Encode the concept descriptions\n",
    "print(\"Encoding concept descriptions...\")\n",
    "concept_embeddings = {name: embedder.encode(desc, convert_to_tensor=True) for name, desc in concepts.items()}\n",
    "\n",
    "# Encode lyrics\n",
    "print(f\"Encoding lyrics for {len(df)} songs...\")\n",
    "lyrics_list = df['lyrics_clean'].tolist()\n",
    "# Batch encoding\n",
    "lyric_embeddings = embedder.encode(lyrics_list, batch_size=64, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Compute cosine similarities\n",
    "print(\"Computing similarities...\")\n",
    "similarity_scores = {}\n",
    "\n",
    "for name, concept_emb in concept_embeddings.items():\n",
    "    # util.cos_sim returns (1, N)\n",
    "    scores = util.cos_sim(concept_emb, lyric_embeddings)[0]\n",
    "    similarity_scores[f'sim_{name}'] = scores.cpu().numpy()\n",
    "\n",
    "# Add to dataframe\n",
    "for name, scores in similarity_scores.items():\n",
    "    df[name] = scores\n",
    "\n",
    "print(\"Semantic similarity computation complete.\")\n",
    "df[['sim_econ_anxiety', 'sim_resilience', 'sim_escapist_party', 'sim_luxury_flex']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb82f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualize Feature Trends (2003â€“2015 Focus)\n",
    "\n",
    "# Define periods for visualization\n",
    "df['period'] = pd.cut(df['year'], \n",
    "                      bins=[1999, 2007, 2010, 2015, 2023], \n",
    "                      labels=['Pre-Recession', 'Recession', 'Post-Recession', 'Modern'])\n",
    "\n",
    "# Aggregate by year\n",
    "yearly_means = df.groupby('year').mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Plotting Function\n",
    "def plot_trend(feature, title, ylabel):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot full trend\n",
    "    sns.lineplot(data=yearly_means, x='year', y=feature, marker='o', linewidth=2, label='Yearly Mean')\n",
    "    \n",
    "    # Highlight Recession Window\n",
    "    plt.axvspan(2008, 2010, color='red', alpha=0.1, label='Recession (2008-2010)')\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Plot Key Features\n",
    "plot_trend('valence_transformer', 'Average Song Valence (Sentiment) Over Time', 'Valence Score')\n",
    "plot_trend('freq_economic', 'Frequency of Economic Terms', 'Frequency')\n",
    "plot_trend('sim_econ_anxiety', 'Semantic Similarity to \"Economic Anxiety\"', 'Cosine Similarity')\n",
    "plot_trend('sim_escapist_party', 'Semantic Similarity to \"Escapist Partying\"', 'Cosine Similarity')\n",
    "plot_trend('sim_luxury_flex', 'Semantic Similarity to \"Luxury/Wealth\"', 'Cosine Similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Statistical Analysis: Detecting Shifts in the Recession Window\n",
    "\n",
    "# Define Dummy Variables\n",
    "df['is_recession'] = df['year'].apply(lambda x: 1 if 2008 <= x <= 2010 else 0)\n",
    "df['is_post_recession'] = df['year'].apply(lambda x: 1 if 2011 <= x <= 2015 else 0)\n",
    "\n",
    "# Function to run OLS\n",
    "def run_recession_regression(target_col):\n",
    "    # Formula: target ~ is_recession + is_post_recession + year + rank\n",
    "    # We center 'year' to avoid multicollinearity issues or large numbers\n",
    "    df['year_centered'] = df['year'] - 2008\n",
    "    \n",
    "    formula = f\"{target_col} ~ is_recession + is_post_recession + year_centered + rank\"\n",
    "    model = smf.ols(formula=formula, data=df).fit()\n",
    "    \n",
    "    print(f\"--- Regression Results for {target_col} ---\")\n",
    "    print(model.summary().tables[1])\n",
    "    print(\"\\n\")\n",
    "    return model\n",
    "\n",
    "# Run for key features\n",
    "key_features = ['valence_transformer', 'freq_economic', 'sim_econ_anxiety', 'sim_escapist_party', 'sim_luxury_flex']\n",
    "\n",
    "for feature in key_features:\n",
    "    run_recession_regression(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Train Recession-Era Classifier\n",
    "\n",
    "# Define Labels\n",
    "# 1 for 2008-2010, 0 for surrounding years (2003-2007 and 2011-2015)\n",
    "# We filter the dataset to this window for training to avoid noise from very old or very new songs\n",
    "train_window_df = df[(df['year'] >= 2003) & (df['year'] <= 2015)].copy()\n",
    "train_window_df['label'] = train_window_df['year'].apply(lambda x: 1 if 2008 <= x <= 2010 else 0)\n",
    "\n",
    "print(f\"Training set size: {len(train_window_df)}\")\n",
    "print(\"Class distribution:\", train_window_df['label'].value_counts())\n",
    "\n",
    "# Select Features\n",
    "features = [\n",
    "    'valence_transformer', \n",
    "    'emotion_sadness', 'emotion_fear', 'emotion_joy',\n",
    "    'freq_economic', 'freq_self', 'freq_social', 'freq_negation', 'ttr', 'avg_word_len',\n",
    "    'sim_econ_anxiety', 'sim_resilience', 'sim_escapist_party', 'sim_luxury_flex'\n",
    "]\n",
    "\n",
    "X = train_window_df[features]\n",
    "y = train_window_df['label']\n",
    "\n",
    "# Standardize Features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "scores = cross_val_score(log_reg, X_scaled, y, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f\"Cross-Validation ROC-AUC: {scores.mean():.3f} (+/- {scores.std():.3f})\")\n",
    "\n",
    "# Fit on full training window\n",
    "log_reg.fit(X_scaled, y)\n",
    "\n",
    "# Inspect Coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient': log_reg.coef_[0]\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\n--- Feature Importance (Logistic Regression Coefficients) ---\")\n",
    "print(coef_df)\n",
    "\n",
    "# Visualize Coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=coef_df, x='Coefficient', y='Feature', palette='viridis')\n",
    "plt.title('Feature Importance for Detecting Recession-Era Songs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf3a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Build and Monitor the Recession-Pop Index\n",
    "\n",
    "# Apply to Full Dataset (1995-2023)\n",
    "# We need to scale the full dataset features using the SAME scaler fitted on the training window\n",
    "X_full = df[features]\n",
    "X_full_scaled = scaler.transform(X_full)\n",
    "\n",
    "# Predict Probabilities\n",
    "df['recession_prob'] = log_reg.predict_proba(X_full_scaled)[:, 1]\n",
    "\n",
    "# Create Index: Mean Probability per Year\n",
    "recession_index = df.groupby('year')['recession_prob'].mean().reset_index()\n",
    "\n",
    "# Plot Index\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.lineplot(data=recession_index, x='year', y='recession_prob', marker='o', linewidth=3, color='darkred')\n",
    "\n",
    "# Highlight Recessions\n",
    "# 2001 (Dot-com), 2008-2009 (Great Recession), 2020 (COVID)\n",
    "plt.axvspan(2001, 2001.9, color='gray', alpha=0.2, label='Recession (NBER)')\n",
    "plt.axvspan(2008, 2009.9, color='gray', alpha=0.2)\n",
    "plt.axvspan(2020, 2020.5, color='gray', alpha=0.2)\n",
    "\n",
    "plt.title('The \"Recession Pop\" Index (2000-2023)', fontsize=18)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Recession Probability (Index)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add smoothing\n",
    "recession_index['smoothed'] = recession_index['recession_prob'].rolling(window=3, center=True).mean()\n",
    "plt.plot(recession_index['year'], recession_index['smoothed'], linestyle='--', color='black', alpha=0.5, label='3-Year Moving Avg')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Check recent years\n",
    "recent = recession_index[recession_index['year'] >= 2018]\n",
    "print(\"Recent Index Values:\")\n",
    "print(recent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PENPAL Kernel",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
