{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e641b620",
   "metadata": {},
   "source": [
    "# Recession Pop Analysis: Dynamic Topic Modeling with KeyNMF\n",
    "\n",
    "This notebook performs a quantitative analysis of \"recession pop\" themes over time (2000-2025) using the `turftopic` library and `KeyNMF`. We will analyze the evolution of topics in pop lyrics.\n",
    "\n",
    "## 1. Install and Import Dependencies\n",
    "First, we ensure all necessary libraries are installed and imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from turftopic import KeyNMF, ClusteringTopicModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "# Set plot style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d142f4f",
   "metadata": {},
   "source": [
    "## 2. Download and Load Dataset\n",
    "We use `kagglehub` to download the dataset and load it into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version of the new dataset\n",
    "path = kagglehub.dataset_download(\"suparnabiswas/billboard-hot-1002000-2023-data-with-features\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# Find the CSV file\n",
    "csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(\"No CSV file found in the dataset.\")\n",
    "    \n",
    "# Assuming the main file is the one we want. \n",
    "csv_path = os.path.join(path, csv_files[0])\n",
    "print(f\"Loading file: {csv_files[0]}\")\n",
    "\n",
    "# Load the dataset\n",
    "# Try-except block to handle potential encoding issues which are common with lyrics datasets\n",
    "try:\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(csv_path, encoding='latin1')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadab321",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Filtering\n",
    "We filter the data to include only tracks from 2000 to 2025, handle missing values, and ensure the 'release_date' is in datetime format. We will also focus on the 'Pop' genre if available or general analysis if not strictly specified, but given the prompt \"recession pop\", we should check the genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289b584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the dataset to ensure we start fresh every time this cell is run\n",
    "if 'csv_path' in locals():\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(csv_path, encoding='latin1')\n",
    "else:\n",
    "    # Fallback if csv_path is lost\n",
    "    path = kagglehub.dataset_download(\"suparnabiswas/billboard-hot-1002000-2023-data-with-features\")\n",
    "    csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "    csv_path = os.path.join(path, csv_files[0])\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(csv_path, encoding='latin1')\n",
    "\n",
    "print(f\"Initial shape: {df.shape}\")\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Check for required columns\n",
    "# The user specified: \"year\", \"lyrics\", \"danceability\", \"ranking\"\n",
    "# Let's check what we actually have (based on previous cell output, but we'll be robust)\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "\n",
    "if 'lyrics' not in df.columns:\n",
    "    raise ValueError(\"Column 'lyrics' not found in dataset.\")\n",
    "if 'year' not in df.columns:\n",
    "    # Try to find a date column if year is missing\n",
    "    print(\"Column 'year' not found. Looking for alternatives...\")\n",
    "    # Add logic here if needed, but assuming 'year' exists as per prompt\n",
    "    pass\n",
    "\n",
    "# Drop rows with missing lyrics or year\n",
    "df = df.dropna(subset=['lyrics', 'year'])\n",
    "\n",
    "# Filter for years 2000-2025\n",
    "# Ensure year is numeric\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "df = df.dropna(subset=['year'])\n",
    "df['year'] = df['year'].astype(int)\n",
    "\n",
    "df = df[(df['year'] >= 2000) & (df['year'] <= 2025)]\n",
    "\n",
    "# Create a datetime column for Turftopic (it expects datetime objects)\n",
    "# We'll use Jan 1st of each year as the timestamp\n",
    "df['release_date'] = pd.to_datetime(df['year'], format='%Y')\n",
    "\n",
    "# Sort by date\n",
    "df = df.sort_values('release_date')\n",
    "\n",
    "print(f\"Filtered dataset shape: {df.shape}\")\n",
    "\n",
    "if df.empty:\n",
    "    print(\"WARNING: The dataframe is empty after filtering!\")\n",
    "    corpus = []\n",
    "    timestamps = []\n",
    "else:\n",
    "    # Prepare corpus and timestamps\n",
    "    corpus = df['lyrics'].tolist()\n",
    "    timestamps = df['release_date'].tolist()\n",
    "\n",
    "    print(f\"Number of documents: {len(corpus)}\")\n",
    "    print(f\"Time range: {min(timestamps)} to {max(timestamps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d501b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data distribution over time to investigate gaps (e.g., 2008)\n",
    "plt.figure(figsize=(12, 6))\n",
    "year_counts = df['release_date'].dt.year.value_counts().sort_index()\n",
    "sns.barplot(x=year_counts.index, y=year_counts.values, color='skyblue')\n",
    "plt.title(\"Number of Songs per Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Check specifically for 2008 and surrounding years\n",
    "print(\"Counts for years around 2008:\")\n",
    "print(year_counts.loc[2006:2010])\n",
    "\n",
    "if year_counts.get(2008, 0) == 0:\n",
    "    print(\"\\nWARNING: No data found for 2008! This explains the gap.\")\n",
    "else:\n",
    "    print(f\"\\nData exists for 2008 ({year_counts[2008]} songs). The gap might be due to binning or model fitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4284b3",
   "metadata": {},
   "source": [
    "## 4. Initialize KeyNMF Topic Model\n",
    "We initialize the `KeyNMF` model. We'll use a standard sentence transformer model for embeddings (default in turftopic is usually 'all-MiniLM-L6-v2' or similar). We set `n_features` (number of keywords) and `n_components` (number of topics). Let's start with 10 topics for overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db524c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KeyNMF\n",
    "# n_components: number of topics\n",
    "# top_n: number of keywords to describe each topic\n",
    "model = KeyNMF(n_components=10, top_n=10, random_state=42)\n",
    "print(\"Model initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7c31e3",
   "metadata": {},
   "source": [
    "## 5. Fit Dynamic Topic Model\n",
    "We use `fit_transform_dynamic` to fit the model and analyze topics over time. We'll bin the data by year (approx 25 bins for 25 years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6507fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have data before fitting\n",
    "if not corpus:\n",
    "    raise ValueError(\"Corpus is empty. Cannot fit model. Please check the 'Data Preprocessing and Filtering' step.\")\n",
    "\n",
    "# Fit the dynamic model\n",
    "# bins=25 creates roughly yearly bins for the 25-year period\n",
    "# This might take some time as it computes embeddings and fits the model\n",
    "print(f\"Fitting model on {len(corpus)} documents...\")\n",
    "document_topic_matrix = model.fit_transform_dynamic(corpus, timestamps=timestamps, bins=25)\n",
    "\n",
    "print(\"Model fitting complete.\")\n",
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228c803e",
   "metadata": {},
   "source": [
    "## 6. Visualize Topics Over Time\n",
    "We visualize the evolution of topics using `plot_topics_over_time()`. This creates an interactive plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e96a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot topics over time\n",
    "fig = model.plot_topics_over_time()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617bc877",
   "metadata": {},
   "source": [
    "## 7. Analyze Specific Topic Trends\n",
    "We can extract the underlying data to perform custom analysis, focusing on themes related to \"recession pop\" (e.g., escapism, partying, money, or economic struggle). Note: You will need to identify which topic ID corresponds to these themes from the previous output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access temporal importance\n",
    "# temporal_importance_ is usually available after fit_transform_dynamic\n",
    "# It might be stored in the model or returned. \n",
    "# Based on docs, model.temporal_importance_ should exist.\n",
    "\n",
    "if hasattr(model, 'temporal_importance_'):\n",
    "    temporal_importance = model.temporal_importance_\n",
    "    \n",
    "    # Create a DataFrame for plotting\n",
    "    # We need time labels. model.bin_edges_ might give us the time points.\n",
    "    # Or we can use the binning logic to recreate labels.\n",
    "    \n",
    "    # Let's try to inspect the structure if available, otherwise we rely on the plot_topics_over_time data\n",
    "    # Actually, plot_topics_over_time returns a figure, but we can also get the data.\n",
    "    # topic_data = model.prepare_dynamic_topic_data(corpus, timestamps, bins=25)\n",
    "    # But we already ran fit_transform_dynamic.\n",
    "    \n",
    "    # Let's assume we want to plot all topics or a subset.\n",
    "    # We can use the data from the model directly if exposed.\n",
    "    \n",
    "    # For demonstration, let's try to access the data used for plotting\n",
    "    pass\n",
    "else:\n",
    "    print(\"Temporal importance attribute not found directly. Using prepare_dynamic_topic_data to get structured data.\")\n",
    "    topic_data = model.prepare_dynamic_topic_data(corpus, timestamps=timestamps, bins=25)\n",
    "    \n",
    "    # topic_data is a TopicData object, likely has a way to get the dataframe\n",
    "    # It usually has 'temporal_importance' and 'time_labels'\n",
    "    \n",
    "    # Let's print available keys/attributes\n",
    "    # print(dir(topic_data)) \n",
    "    \n",
    "    # Assuming we can get a dataframe for custom plotting\n",
    "    # This part depends on the exact structure of TopicData which is not fully detailed in the snippet,\n",
    "    # but usually it's straightforward.\n",
    "    pass\n",
    "\n",
    "# Example of custom plotting if we had the data in a DF 'temporal_df' with columns 'Topic', 'Time', 'Importance'\n",
    "# sns.lineplot(data=temporal_df, x='Time', y='Importance', hue='Topic')\n",
    "# plt.title(\"Topic Importance Over Time\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec87349e",
   "metadata": {},
   "source": [
    "## 8. Alternative: Clustering Topic Model\n",
    "We can also use a clustering-based approach (`ClusteringTopicModel`). This fits a global model on the whole corpus and then estimates term importances over time slices. This can sometimes provide more stable topics than matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d62ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ClusteringTopicModel\n",
    "# n_reduce_to: number of topics (similar to n_components)\n",
    "clustering_model = ClusteringTopicModel(n_reduce_to=10, top_n=10, random_state=42)\n",
    "print(\"ClusteringTopicModel initialized.\")\n",
    "\n",
    "# Fit the dynamic clustering model\n",
    "print(f\"Fitting ClusteringTopicModel on {len(corpus)} documents...\")\n",
    "# Using the same bins as before\n",
    "clustering_doc_topic_matrix = clustering_model.fit_transform_dynamic(corpus, timestamps=timestamps, bins=25)\n",
    "\n",
    "print(\"Clustering model fitting complete.\")\n",
    "clustering_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd129d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot topics over time for Clustering Model\n",
    "fig_clustering = clustering_model.plot_topics_over_time()\n",
    "fig_clustering.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada2e22",
   "metadata": {},
   "source": [
    "## 9. Sentiment Analysis Over Time\n",
    "We will now analyze the evolution of sentiment in the lyrics over time using a pre-trained Transformer model (`cardiffnlp/twitter-roberta-base-sentiment-latest`). We will compute a continuous valence score for each song and aggregate it by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"Get the appropriate device (MPS for Mac, CUDA, or CPU).\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def load_sentiment_model(model_name: str, device: torch.device):\n",
    "    print(f\"Loading sentiment model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return tokenizer, model\n",
    "\n",
    "def continuous_valence_score(probs: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert sentiment probabilities to continuous valence scores.\n",
    "    Assumes 3-class model: [Negative, Neutral, Positive] (0, 1, 2)\n",
    "    Score = P(Positive) - P(Negative)\n",
    "    Range: [-1, 1]\n",
    "    \"\"\"\n",
    "    # probs shape: (batch_size, 3)\n",
    "    # col 0: Negative, col 1: Neutral, col 2: Positive\n",
    "    valence = probs[:, 2] - probs[:, 0]\n",
    "    return valence\n",
    "\n",
    "def compute_sentiment_batch(\n",
    "    texts: list[str],\n",
    "    model_name: str = \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    batch_size: int = 32,\n",
    "    device: torch.device = None\n",
    ") -> np.ndarray:\n",
    "    \n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    tokenizer, model = load_sentiment_model(model_name, device)\n",
    "    \n",
    "    all_scores = []\n",
    "    print(f\"Computing sentiment for {len(texts)} texts (batch_size={batch_size})...\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Sentiment batches\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512 \n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            valence = continuous_valence_score(probs)\n",
    "            all_scores.extend(valence.cpu().numpy())\n",
    "            \n",
    "    return np.array(all_scores)\n",
    "\n",
    "# Define the model to use\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b3ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentiment for the corpus\n",
    "# We use the 'corpus' list which contains the lyrics\n",
    "# Note: This might take a while depending on the dataset size and hardware\n",
    "sentiment_scores = compute_sentiment_batch(corpus, model_name=MODEL_NAME, batch_size=32)\n",
    "\n",
    "# Add scores to the dataframe\n",
    "# Ensure the dataframe aligns with the corpus (it should, as we created corpus from df)\n",
    "df['sentiment_score'] = sentiment_scores\n",
    "\n",
    "print(\"Sentiment computation complete.\")\n",
    "df[['track_name', 'release_date', 'sentiment_score']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5cb822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Sentiment Over Time\n",
    "# Aggregate by year\n",
    "df['year'] = df['release_date'].dt.year\n",
    "yearly_sentiment = df.groupby('year')['sentiment_score'].mean().reset_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=yearly_sentiment, x='year', y='sentiment_score', marker='o', linewidth=2.5, color='purple')\n",
    "plt.title(\"Average Lyric Sentiment Over Time (2000-2025)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Average Sentiment Score (Valence)\")\n",
    "plt.axhline(0, color='gray', linestyle='--', alpha=0.5) # Zero line for neutral\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Interactive Plot with Plotly\n",
    "fig = px.line(yearly_sentiment, x='year', y='sentiment_score', \n",
    "              title='Average Lyric Sentiment Over Time (2000-2025)',\n",
    "              labels={'sentiment_score': 'Average Sentiment (Valence)', 'year': 'Year'},\n",
    "              markers=True)\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", annotation_text=\"Neutral\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae23fa4",
   "metadata": {},
   "source": [
    "## 10. Linear Regression: Sentiment ~ Year\n",
    "Finally, we fit a linear model to statistically test if there is a significant trend in sentiment over the years. We will use `scipy.stats.linregress` to obtain the slope, p-value, and other statistics, and visualize the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data\n",
    "# We use the full dataset to capture the variance\n",
    "# Transform Year to Log-Space (Log of time elapsed)\n",
    "# This models a relationship where changes happen fast initially and slow down (or vice versa)\n",
    "# We add 1 to avoid log(0)\n",
    "df['log_time'] = np.log(df['year'] - df['year'].min() + 1)\n",
    "\n",
    "x = df['log_time']\n",
    "y = df['sentiment_score']\n",
    "\n",
    "# Perform linear regression on log-transformed time\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "print(f\"Linear Regression Results (Sentiment ~ Log(Time)):\")\n",
    "print(f\"Slope (Coefficient): {slope:.5f}\")\n",
    "print(f\"Intercept: {intercept:.5f}\")\n",
    "print(f\"R-squared: {r_value**2:.5f}\")\n",
    "print(f\"P-value: {p_value:.5e}\")\n",
    "print(f\"Standard Error: {std_err:.5f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"\\nResult: There is a statistically significant relationship between log(time) and sentiment (p < 0.05).\")\n",
    "    if slope > 0:\n",
    "        print(\"Trend: Sentiment is increasing logarithmically over time.\")\n",
    "    else:\n",
    "        print(\"Trend: Sentiment is decreasing logarithmically over time.\")\n",
    "else:\n",
    "    print(\"\\nResult: There is NO statistically significant relationship between log(time) and sentiment (p >= 0.05).\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Scatter plot\n",
    "sns.scatterplot(x='year', y='sentiment_score', data=df, alpha=0.1, s=10, color='blue', label='Data')\n",
    "\n",
    "# Generate points for the fitted curve\n",
    "x_range = np.linspace(df['year'].min(), df['year'].max(), 100)\n",
    "log_x_range = np.log(x_range - df['year'].min() + 1)\n",
    "y_pred = slope * log_x_range + intercept\n",
    "\n",
    "plt.plot(x_range, y_pred, color='red', linewidth=2, label=f'Log Fit: y={slope:.2f}*log(t) + {intercept:.2f}')\n",
    "\n",
    "plt.title(f\"Regression: Sentiment ~ Log(Time)\\n(Slope: {slope:.4f}, p={p_value:.2e})\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Sentiment Score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c840059",
   "metadata": {},
   "source": [
    "## 11. Hypothesis Testing: Escapism vs. Vulnerability (Zero-Shot Classification)\n",
    "To directly test the hypothesis that lyrics are moving from **Escapism** to **Vulnerability/Anxiety**, we use a Zero-Shot Classifier. This allows us to score the lyrics against these specific concepts without needing a labeled dataset.\n",
    "\n",
    "We will use `facebook/bart-large-mnli` (or a distilled version for speed) to classify lyrics into:\n",
    "*   `Escapism` / `Partying`\n",
    "*   `Vulnerability` / `Anxiety` / `Depression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780f2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize Zero-Shot Classifier\n",
    "# We use a distilled model for faster inference on CPU/MPS\n",
    "# If you have a powerful GPU, you can use \"facebook/bart-large-mnli\"\n",
    "ZERO_SHOT_MODEL = \"facebook/bart-large-mnli\" \n",
    "device = get_device()\n",
    "\n",
    "print(f\"Loading Zero-Shot model: {ZERO_SHOT_MODEL} on {device}\")\n",
    "classifier = pipeline(\"zero-shot-classification\", model=ZERO_SHOT_MODEL, device=device)\n",
    "\n",
    "# Define hypothesis labels\n",
    "candidate_labels = [\"escapism\", \"partying\", \"vulnerability\", \"anxiety\", \"depression\"]\n",
    "\n",
    "# Run classification on a sample (or full dataset if time permits)\n",
    "# Zero-shot is computationally expensive. Let's sample 500 songs per 5-year block or just run on a subset for demo.\n",
    "# For this exam/demo, let's take a random sample of 1000 songs to keep it quick.\n",
    "SAMPLE_SIZE = 1000\n",
    "if len(df) > SAMPLE_SIZE:\n",
    "    print(f\"Dataset is large. Sampling {SAMPLE_SIZE} songs for Zero-Shot analysis...\")\n",
    "    df_sample = df.sample(n=SAMPLE_SIZE, random_state=42).copy()\n",
    "else:\n",
    "    df_sample = df.copy()\n",
    "\n",
    "print(\"Classifying lyrics...\")\n",
    "# We process in batches to be safe\n",
    "batch_size = 16\n",
    "results = []\n",
    "\n",
    "# Prepare text: Truncate to fit model context (usually 512 or 1024 tokens)\n",
    "# Lyrics can be long, so we take the first 512 chars as a proxy or use truncation in pipeline\n",
    "texts = df_sample['lyrics'].astype(str).tolist()\n",
    "\n",
    "for i in tqdm(range(0, len(texts), batch_size), desc=\"Zero-Shot Batches\"):\n",
    "    batch_texts = texts[i:i+batch_size]\n",
    "    batch_results = classifier(batch_texts, candidate_labels, multi_label=True, truncation=True)\n",
    "    results.extend(batch_results)\n",
    "\n",
    "# Extract scores\n",
    "# Each result has 'labels' and 'scores'. We need to map them back.\n",
    "scores_dict = {label: [] for label in candidate_labels}\n",
    "\n",
    "for res in results:\n",
    "    for label, score in zip(res['labels'], res['scores']):\n",
    "        scores_dict[label].append(score)\n",
    "\n",
    "# Add to sample dataframe\n",
    "for label in candidate_labels:\n",
    "    df_sample[f'score_{label}'] = scores_dict[label]\n",
    "\n",
    "print(\"Classification complete.\")\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b37d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Hypothesis Trends\n",
    "# Group by year\n",
    "df_sample['year'] = df_sample['release_date'].dt.year\n",
    "yearly_scores = df_sample.groupby('year')[[f'score_{label}' for label in candidate_labels]].mean().reset_index()\n",
    "\n",
    "# Melt for plotting\n",
    "yearly_scores_melted = yearly_scores.melt(id_vars=['year'], var_name='Concept', value_name='Score')\n",
    "yearly_scores_melted['Concept'] = yearly_scores_melted['Concept'].str.replace('score_', '').str.capitalize()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.lineplot(data=yearly_scores_melted, x='year', y='Score', hue='Concept', marker='o', linewidth=2)\n",
    "\n",
    "plt.title(\"Evolution of Lyrical Themes: Escapism vs. Vulnerability (2000-2025)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Zero-Shot Classification Score (Probability)\")\n",
    "plt.legend(title=\"Theme\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Interactive Plot\n",
    "fig = px.line(yearly_scores_melted, x='year', y='Score', color='Concept',\n",
    "              title='Evolution of Lyrical Themes: Escapism vs. Vulnerability (2000-2025)',\n",
    "              markers=True)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
